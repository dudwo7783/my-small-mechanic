{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document를 보고 질문에 대한 정답 document 인덱스를 만들어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import openai\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델이름\n",
    "company = 'OPENAI'\n",
    "embedding_models = 'text-embedding-3-large'\n",
    "header_include = True\n",
    "table_include = True\n",
    "split_yn = False\n",
    "documnet_name = \"2024 IONIQ5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f\"{company}_{embedding_models}_h{header_include}_t{table_include}_s{split_yn}\"\n",
    "exp_dir = '../experiment/result/' + exp_name\n",
    "\n",
    "embedding_result_path = exp_dir + '/embeddings.parquet'\n",
    "df = pd.read_parquet(embedding_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['h1']!=df['doc_contents']]\n",
    "df = df[df['h2']!=df['doc_contents']]\n",
    "df = df[df['h3']!=df['doc_contents']]\n",
    "\n",
    "df = df.reset_index(drop=True).reset_index()\n",
    "\n",
    "#NaN 처리\n",
    "df[['table_contents']] = df[['table_contents']].fillna('')\n",
    "df['img_urls'] = df['img_urls'].apply(lambda d: d.tolist() if d is not None else [])\n",
    "df['table_img_urls'] = df['table_img_urls'].apply(lambda d: d.tolist() if d is not None else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM을 활용한 Evaluation Test Set 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(api_key=os.environ['OPENAI_API_KEY'], temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_prompt_template = \"\"\"\n",
    "\\n\\nHuman: Here is the context information, inside <context></context> XML tags.\n",
    "Please don't make questions with the contents of the table.\n",
    "\n",
    "<chapter>{chapter}</chapter>Given the chapter name\n",
    "<majorheading>{majorheading}</majorheading>Given the major heading of chapter.\n",
    "<minorheading>{minorheading}</minorheading>Given the minor heading of chapter.\n",
    "<context>{context}</context>Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "You are a Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\quiz/examination.\n",
    "The questions should be diverse in nature \\across the document.\n",
    "The questions should not contain options, start with \"-\"\n",
    "Restrict the questions to the context information provided.\n",
    "Write in Korean. \n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "PROMPT_RETRIEVER = PromptTemplate(\n",
    "    template=retriever_prompt_template,\n",
    "    input_variables=[\"context\", \"num_questions_per_chunk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_prompt_template = \"\"\"\n",
    "Here is the context, inside <context></context> XML tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Only using the context as above, answer the following question with the rules as below:\n",
    "    - Don't insert XML tag such as <context> and </context> when answering.\n",
    "    - Write as much as you can\n",
    "    - Be courteous and polite\n",
    "    - Only answer the question if you can find the answer in the context with certainty.\n",
    "    - Skip the preamble\n",
    "    - Use three sentences maximum and keep the answer concise.\n",
    "    - If the answer is not in the context, just say \"Could not find answer in given contexts.\"\n",
    "    - The each answers should start with \"-\"\n",
    "    - Answer in Korean.\n",
    "Question:\n",
    "{question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT_GENERATION = PromptTemplate(\n",
    "    template=generation_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "\n",
    "chain1 = LLMChain(llm=llm,prompt=PROMPT_RETRIEVER,output_key=\"question\",verbose=False)\n",
    "chain2 = LLMChain(llm=llm,prompt=PROMPT_GENERATION,output_key=\"answer\", verbose=False)\n",
    "\n",
    "chain = SequentialChain(chains=[chain1,chain2],\n",
    "                        input_variables=[\"chapter\", \"majorheading\", \"minorheading\", \"context\",\"num_questions_per_chunk\"],\n",
    "                        output_variables=['context', 'question','answer'],verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = ''\n",
    "df['answer'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_qa_from_pandas(pdf):\n",
    "    for i, row in tqdm(pdf.iterrows()):\n",
    "        chapter = row['h1']\n",
    "        majorheading = row['h2']\n",
    "        minorheading = row['h3']\n",
    "        context = row['doc_contents']\n",
    "        if len(context)<512:\n",
    "            num_questions_per_chunk = 1\n",
    "        elif (len(context)>=512) and (len(context)<1024):\n",
    "            num_questions_per_chunk = 2\n",
    "        else:\n",
    "            num_questions_per_chunk = 3\n",
    "        \n",
    "        result = chain({'chapter': chapter, 'majorheading': majorheading, 'minorheading': minorheading, 'context': context,'num_questions_per_chunk':num_questions_per_chunk})\n",
    "        question = result['question']\n",
    "        answer = result['answer']\n",
    "            \n",
    "        df.loc[i, 'question'] = question\n",
    "        df.loc[i, 'answer'] = answer\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:50,  5.05s/it]\n"
     ]
    }
   ],
   "source": [
    "bad_request_index = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    chapter = row['h1']\n",
    "    majorheading = row['h2']\n",
    "    minorheading = row['h3']\n",
    "    context = row['doc_contents']\n",
    "    table_img_urls = row['table_img_urls']\n",
    "    if not table_img_urls:\n",
    "        if len(context)<512:\n",
    "            num_questions_per_chunk = 1\n",
    "        elif (len(context)>=512) and (len(context)<1024):\n",
    "            num_questions_per_chunk = 2\n",
    "        else:\n",
    "            num_questions_per_chunk = 3\n",
    "        \n",
    "        result = chain({'chapter': chapter, 'majorheading': majorheading, 'minorheading': minorheading, 'context': context,'num_questions_per_chunk':num_questions_per_chunk})\n",
    "        question = result['question']\n",
    "        answer = result['answer']\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    df.loc[i, 'question'] = question\n",
    "    df.loc[i, 'answer'] = answer\n",
    "    if i==10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 전기 자동차의 주요 장치 중 하나인 OBC는 무엇을 하는 장치인가요?\n",
      "- 고전압 부품 및 구동용(고전압) 배터리를 분리하거나 손상시키면 어떤 위험성이 있을까요?\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[3]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- OBC는 구동용(고전압) 배터리를 충전하는 장치입니다.\n",
      "- 고전압 부품 및 구동용(고전압) 배터리를 분리하거나 손상시키면 감전 등의 사고가 발생해 심각한 부상을 입을 수 있으며 차량의 성능 및 내구에 영향을 줄 수 있습니다. \n",
      "- 고전압 부품 및 구동용(고전압) 배터리의 점검 및 정비가 필요할 경우 당사 직영하이테크센터나 블루핸즈에서 점검을 받아야 합니다.\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[3]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['h1', 'h2', 'h3', 'doc_contents', 'question', 'answer']].loc[:10].to_csv(\"~/test.csv\",index=False, encoding='cp949')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS를 위한 custom dataloader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(df, page_content_column=\"doc_table_contents\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 임베딩 모델 설정\n",
    "embedding_model = OpenAIEmbeddings(model=embedding_models,\n",
    "                                  openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)  # Modify model_name if you have access to GPT-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=llm,\n",
    "    critic_llm=llm,\n",
    "    embeddings=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "\n",
    "test_df = testset.to_pandas()\n",
    "test_questions = test_df[\"question\"].values.tolist()\n",
    "test_groundtruths = test_df[\"ground_truth\"].values.tolist()\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myMechanic",
   "language": "python",
   "name": "mymechanic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
